{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766248dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install transformers timm einops opencv-python matplotlib scikit-image --upgrade\n",
    "# %pip install segmentation-models-pytorch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import segmentation_models_pytorch as smp\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import Normalize\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8eba6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_DATA_DIR = \"./DIV2K_train_HR\"\n",
    "CROPPED_DATA_DIR = \"./cropped_dataset\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434c2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'DIV2K_train_HR' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_and_unzip_div2k():\n",
    "    \"\"\"\n",
    "    Downloads and unzips the DIV2K high-resolution training dataset.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    dataset_url = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
    "    download_dir = \"./\"\n",
    "    dataset_name = \"DIV2K_train_HR\"\n",
    "    zip_path = os.path.join(download_dir, f\"{dataset_name}.zip\")\n",
    "    \n",
    "    # --- Check if dataset already exists ---\n",
    "    if os.path.exists(os.path.join(download_dir, dataset_name)):\n",
    "        print(f\"Dataset '{dataset_name}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Download the file with a progress bar ---\n",
    "    print(f\"Downloading {dataset_name}.zip... (This may take a while)\")\n",
    "    try:\n",
    "        response = requests.get(dataset_url, stream=True)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        \n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 # 1 Kibibyte\n",
    "        \n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "\n",
    "        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "            print(\"ERROR, something went wrong during download.\")\n",
    "            return\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Unzip the file ---\n",
    "    print(f\"\\nUnzipping {zip_path}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)\n",
    "        print(\"Unzipping complete.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The downloaded file is not a valid zip file or is corrupted.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Clean up the zip file ---\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Cleaned up {zip_path}.\")\n",
    "    print(\"--- Dataset setup is complete! ---\")\n",
    "\n",
    "# --- Run the setup function ---\n",
    "download_and_unzip_div2k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc6913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test directories seem to already exist. Skipping organization.\n"
     ]
    }
   ],
   "source": [
    "# --- Define the new directories ---\n",
    "TRAIN_SHARP_DIR = os.path.join(ORIGINAL_DATA_DIR, \"train\", \"sharp\")\n",
    "TEST_SHARP_DIR = os.path.join(ORIGINAL_DATA_DIR, \"test\", \"sharp\")\n",
    "\n",
    "def organize_images():\n",
    "    \"\"\"\n",
    "    Moves images from a flat directory into train/sharp and test/sharp subdirectories.\n",
    "    \"\"\"\n",
    "    # Check if the script has already been run\n",
    "    if os.path.exists(TRAIN_SHARP_DIR) or os.path.exists(TEST_SHARP_DIR):\n",
    "        print(\"Train/Test directories seem to already exist. Skipping organization.\")\n",
    "        return\n",
    "\n",
    "    print(\"Organizing sharp images into train/test splits...\")\n",
    "    \n",
    "    # Create the new directories\n",
    "    os.makedirs(TRAIN_SHARP_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_SHARP_DIR, exist_ok=True)\n",
    "    \n",
    "    # Get all sharp image files from the source directory\n",
    "    image_files = sorted([f for f in os.listdir(ORIGINAL_DATA_DIR) if f.endswith('.png')])\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"Error: No .png images found in {ORIGINAL_DATA_DIR}. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Split the files: first 700 for training, last 100 for testing\n",
    "    train_files = image_files[:700]\n",
    "    test_files = image_files[700:]\n",
    "\n",
    "    # Move the files\n",
    "    print(f\"Moving {len(train_files)} images to {TRAIN_SHARP_DIR}...\")\n",
    "    for filename in tqdm(train_files):\n",
    "        shutil.move(os.path.join(ORIGINAL_DATA_DIR, filename), os.path.join(TRAIN_SHARP_DIR, filename))\n",
    "        \n",
    "    print(f\"Moving {len(test_files)} images to {TEST_SHARP_DIR}...\")\n",
    "    for filename in tqdm(test_files):\n",
    "        shutil.move(os.path.join(ORIGINAL_DATA_DIR, filename), os.path.join(TEST_SHARP_DIR, filename))\n",
    "\n",
    "    print(\"Image organization complete!\")\n",
    "\n",
    "# --- Run the organization script ---\n",
    "organize_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3747c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blurry images for 'train' set already exist. Skipping generation.\n",
      "Blurry images for 'test' set already exist. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_motion_blur_kernel(size, angle):\n",
    "    \"\"\"Generates a linear motion blur kernel.\"\"\"\n",
    "    kernel = np.zeros((size, size))\n",
    "    center = (size - 1) / 2\n",
    "    radian_angle = np.deg2rad(angle)\n",
    "    x_end = round(center + center * np.cos(radian_angle))\n",
    "    y_end = round(center - center * np.sin(radian_angle))\n",
    "    cv2.line(kernel, (round(center), round(center)), (int(x_end), int(y_end)), 1, thickness=1)\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "def generate_defocus_kernel(size, radius):\n",
    "    \"\"\"Generates a defocus (disk) kernel.\"\"\"\n",
    "    kernel = np.zeros((size, size))\n",
    "    center = size // 2\n",
    "    cv2.circle(kernel, (center, center), radius, 1, thickness=-1)\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "def apply_blur_to_directory(source_dir, target_dir):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(source_dir) if f.endswith(('.png'))])\n",
    "    \n",
    "    print(f\"Generating blurry images for {len(image_files)} sharp images...\")\n",
    "    \n",
    "    for filename in tqdm(image_files):\n",
    "        sharp_path = os.path.join(source_dir, filename)\n",
    "        image = cv2.imread(sharp_path)\n",
    "        \n",
    "        # Randomly choose a blur type\n",
    "        blur_type = np.random.choice(['motion', 'gaussian', 'defocus'])\n",
    "        \n",
    "        if blur_type == 'motion':\n",
    "            kernel_size = np.random.randint(15, 35)\n",
    "            kernel_angle = np.random.randint(0, 180)\n",
    "            kernel = generate_motion_blur_kernel(kernel_size, kernel_angle)\n",
    "        elif blur_type == 'defocus':\n",
    "            kernel_size = np.random.randint(15, 35)\n",
    "            radius = np.random.randint(3, kernel_size // 2)\n",
    "            kernel = generate_defocus_kernel(kernel_size, radius)\n",
    "        else: # gaussian\n",
    "            kernel_size = np.random.choice([15, 19, 21, 25])\n",
    "            sigma = np.random.uniform(3, 8)\n",
    "            kernel = cv2.getGaussianKernel(kernel_size, sigma)\n",
    "            kernel = np.dot(kernel, kernel.T)\n",
    "        \n",
    "        blurry_image = cv2.filter2D(image, -1, kernel)\n",
    "        blur_path = os.path.join(target_dir, filename)\n",
    "        cv2.imwrite(blur_path, blurry_image)\n",
    "\n",
    "    print(\"Blurry images generated successfully.\")\n",
    "\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    sharp_folder = os.path.join(ORIGINAL_DATA_DIR, split, 'sharp')\n",
    "    blur_folder = os.path.join(ORIGINAL_DATA_DIR, split, 'blur')\n",
    "    if not os.path.exists(blur_folder) or len(os.listdir(blur_folder)) == 0:\n",
    "        print(f\"Generating blurry images for '{split}' set...\")\n",
    "        apply_blur_to_directory(sharp_folder, blur_folder)\n",
    "    else:\n",
    "        print(f\"Blurry images for '{split}' set already exist. Skipping generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cdd521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped dataset directory already exists and is not empty. Skipping cropping.\n"
     ]
    }
   ],
   "source": [
    "def create_crops(source_base, target_base, crop_size=(224, 224), crops_per_image=10):\n",
    "    \"\"\"Generates and saves random crops from source images.\"\"\"\n",
    "    if os.path.exists(target_base) and len(os.listdir(target_base)) > 0:\n",
    "        print(\"Cropped dataset directory already exists and is not empty. Skipping cropping.\")\n",
    "        return\n",
    "        \n",
    "    print(\"Starting dataset pre-processing with random crops...\")\n",
    "    for split in ['train', 'test']:\n",
    "        sharp_source_dir = os.path.join(source_base, split, 'sharp')\n",
    "        blur_source_dir = os.path.join(source_base, split, 'blur')\n",
    "\n",
    "        if not os.path.exists(sharp_source_dir):\n",
    "            continue\n",
    "\n",
    "        sharp_target_dir = os.path.join(target_base, split, 'sharp')\n",
    "        blur_target_dir = os.path.join(target_base, split, 'blur')\n",
    "        os.makedirs(sharp_target_dir, exist_ok=True)\n",
    "        os.makedirs(blur_target_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing images in: {sharp_source_dir}\")\n",
    "        for filename in tqdm(os.listdir(sharp_source_dir)):\n",
    "            sharp_img = Image.open(os.path.join(sharp_source_dir, filename)).convert(\"RGB\")\n",
    "            blur_img = Image.open(os.path.join(blur_source_dir, filename)).convert(\"RGB\")\n",
    "\n",
    "            img_w, img_h = sharp_img.size\n",
    "            crop_h, crop_w = crop_size\n",
    "            if img_w < crop_w or img_h < crop_h:\n",
    "                continue\n",
    "\n",
    "            for i in range(crops_per_image):\n",
    "                top, left = random.randint(0, img_h - crop_h), random.randint(0, img_w - crop_w)\n",
    "                sharp_cropped = sharp_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "                blur_cropped = blur_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "                \n",
    "                base_name, ext = os.path.splitext(filename)\n",
    "                new_filename = f\"{base_name}_crop_{i}{ext}\"\n",
    "                sharp_cropped.save(os.path.join(sharp_target_dir, new_filename))\n",
    "                blur_cropped.save(os.path.join(blur_target_dir, new_filename))\n",
    "    print(f\"Cropped images saved to: {target_base}\")\n",
    "\n",
    "# --- Run Cropping ---\n",
    "create_crops(ORIGINAL_DATA_DIR, CROPPED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed50532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation directory already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "def create_validation_split(base_dir, ratio=0.1):\n",
    "    \"\"\"Moves a random subset of training images to a new validation directory.\"\"\"\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    \n",
    "    if os.path.exists(val_dir):\n",
    "        print(\"Validation directory already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating validation split...\")\n",
    "    os.makedirs(os.path.join(val_dir, \"sharp\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, \"blur\"), exist_ok=True)\n",
    "\n",
    "    sharp_train_dir = os.path.join(train_dir, \"sharp\")\n",
    "    image_files = sorted(os.listdir(sharp_train_dir))\n",
    "    random.shuffle(image_files)\n",
    "    num_val_images = int(len(image_files) * ratio)\n",
    "    val_images = image_files[:num_val_images]\n",
    "\n",
    "    print(f\"Moving {num_val_images} images from train to validation set...\")\n",
    "    for filename in tqdm(val_images):\n",
    "        shutil.move(os.path.join(train_dir, \"sharp\", filename), os.path.join(val_dir, \"sharp\", filename))\n",
    "        shutil.move(os.path.join(train_dir, \"blur\", filename), os.path.join(val_dir, \"blur\", filename))\n",
    "    print(\"Validation set created successfully.\")\n",
    "\n",
    "# --- Run Validation Split ---\n",
    "create_validation_split(CROPPED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset loaded with 6300 images.\n",
      "Validation dataset loaded with 700 images.\n",
      "Test dataset loaded with 1000 images.\n"
     ]
    }
   ],
   "source": [
    "class DeblurDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading paired deblurring data.\"\"\"\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        self.split_dir = os.path.join(root_dir, split)\n",
    "        self.sharp_dir = os.path.join(self.split_dir, 'sharp')\n",
    "        self.blur_dir = os.path.join(self.split_dir, 'blur')\n",
    "        self.image_files = sorted(os.listdir(self.sharp_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load sharp and blurred images\n",
    "        img_name = self.image_files[idx]\n",
    "        sharp_path = os.path.join(self.sharp_dir, img_name)\n",
    "        blur_path = os.path.join(self.blur_dir, img_name)\n",
    "        \n",
    "        # Use RGB format\n",
    "        sharp_image = Image.open(sharp_path).convert(\"RGB\")\n",
    "        blur_image = Image.open(blur_path).convert(\"RGB\")\n",
    "\n",
    "        # Data augmentation by applying random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            sharp_image = TF.hflip(sharp_image)\n",
    "            blur_image = TF.hflip(blur_image)\n",
    "        \n",
    "        # convert PIL format to tensor format\n",
    "        sharp_image = TF.to_tensor(sharp_image)\n",
    "        blur_image = TF.to_tensor(blur_image)\n",
    "        \n",
    "        return blur_image, sharp_image\n",
    "    \n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='train')\n",
    "val_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='validation')\n",
    "test_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(f\"Training dataset loaded with {len(train_dataset)} images.\")\n",
    "print(f\"Validation dataset loaded with {len(val_dataset)} images.\")\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c45c945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ViTDecoder(nn.Module):\n",
    "    def __init__(self, in_features=768, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(in_features, 256, kernel_size=2, stride=2) # 14x14 -> 28x28\n",
    "        self.conv1 = ConvBlock(256, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 28x28 -> 56x56\n",
    "        self.conv2 = ConvBlock(128, 128)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) # 56x56 -> 112x112\n",
    "        self.conv3 = ConvBlock(64, 64)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) # 112x112 -> 224x224\n",
    "        self.conv4 = ConvBlock(32, 32)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = x[:, 1:, :]\n",
    "        \n",
    "        h = w = int(patches.shape[1]**0.5)\n",
    "        patches = patches.permute(0, 2, 1).contiguous().view(-1, 768, h, w)\n",
    "\n",
    "        x = self.conv1(self.upconv1(patches))\n",
    "        x = self.conv2(self.upconv2(x))\n",
    "        x = self.conv3(self.upconv3(x))\n",
    "        x = self.conv4(self.upconv4(x))\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ViTForDeblurring(nn.Module):\n",
    "    def __init__(self, freeze_encoder=True, num_unfrozen_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.decoder = ViTDecoder()\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(f\"Fine-tuning ViT: Unfreezing the last {num_unfrozen_layers} transformer layers.\")\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for layer in self.encoder.encoder.layer[-num_unfrozen_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            for param in self.encoder.layernorm.parameters():\n",
    "                 param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        last_hidden_state = encoder_output.last_hidden_state\n",
    "        decoded_output = self.decoder(last_hidden_state)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A perceptual loss function based on the VGG19 network.\n",
    "    It computes the L1 loss between the feature maps of the input and target images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        # Use features from a shallower layer (conv3_4) for efficiency\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:18].eval()\n",
    "        self.features = nn.Sequential(*vgg).to(DEVICE)\n",
    "        \n",
    "        # VGG networks require a specific ImageNet normalization\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Freeze the VGG network parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, input_img, target_img):\n",
    "        \"\"\"Computes the perceptual loss.\"\"\"\n",
    "        # Normalize both images before feeding them to VGG\n",
    "        input_norm = self.normalize(input_img)\n",
    "        target_norm = self.normalize(target_img)\n",
    "\n",
    "        # Extract features\n",
    "        input_features = self.features(input_norm)\n",
    "        target_features = self.features(target_norm)\n",
    "        \n",
    "        # Compute the L1 loss between the feature maps\n",
    "        return self.l1(input_features, target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def train_model(model, model_name, train_loader, val_loader, optimizer, epochs, device, lambda_vgg=0.01):\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_vgg = VGGPerceptualLoss().to(device)\n",
    "    model.to(device)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    print(f\"--- Starting Training for {model_name} ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_l1_loss = 0.0\n",
    "        running_vgg_loss = 0.0\n",
    "\n",
    "        for blurry_imgs, sharp_imgs in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            blurry_imgs, sharp_imgs = blurry_imgs.to(device), sharp_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(blurry_imgs)\n",
    "\n",
    "            loss_l1 = criterion_l1(outputs, sharp_imgs)\n",
    "            loss_vgg = criterion_vgg(outputs, sharp_imgs)\n",
    "            total_loss = loss_l1 + (lambda_vgg * loss_vgg)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_l1_loss += loss_l1.item()\n",
    "            running_vgg_loss += loss_vgg.item()\n",
    "        \n",
    "        avg_train_l1 = running_l1_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for blurry_imgs, sharp_imgs in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                blurry_imgs, sharp_imgs = blurry_imgs.to(device), sharp_imgs.to(device)\n",
    "                outputs = model(blurry_imgs)\n",
    "                loss = criterion_l1(outputs, sharp_imgs)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] complete. Train L1: {avg_train_l1:.4f}, Val L1: {avg_val_loss:.4f}, LR: {current_lr}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"--- Finished Training for {model_name} ---\")\n",
    "    torch.save(model.state_dict(), f'{model_name}_deblur.pth')\n",
    "    print(f\"Model saved to {model_name}_deblur.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# --- Initialize U-Net from the segmentation-models-pytorch library ---\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_attention_type='scse'\n",
    ")\n",
    "\n",
    "optimizer_unet = optim.AdamW(unet_model.parameters(), lr=LEARNING_RATE)\n",
    "UNET_MODEL_NAME = \"unet\"\n",
    "print(\"U-Net with ResNet50 backbone created successfully.\")\n",
    "\n",
    "# --- Initialize Transformer model ---\n",
    "vit_model = ViTForDeblurring(freeze_encoder=False)\n",
    "optimizer_vit = optim.Adam([\n",
    "    {'params': vit_model.encoder.parameters(), 'lr': 1e-5},  # Lower LR for encoder\n",
    "    {'params': vit_model.decoder.parameters(), 'lr': 1e-4}   # Higher LR for decoder\n",
    "])\n",
    "VIT_MODEL_NAME = \"vit\"\n",
    "print(\"ViT-based model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train U-Net Baseline ---\n",
    "trained_unet = train_model(unet_model, UNET_MODEL_NAME, train_loader, val_loader, optimizer_unet, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train ViT Model ---\n",
    "trained_vit = train_model(vit_model, VIT_MODEL_NAME, train_loader, val_loader, optimizer_vit, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78493841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_cropped(unet_arch, vit_arch, unet_path, vit_path, test_dataset, device, num_images=10):\n",
    "    \"\"\"Loads both models and evaluates on random cropped test images.\"\"\"\n",
    "    try:\n",
    "        unet_arch.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "        vit_arch.load_state_dict(torch.load(vit_path, weights_only=True))\n",
    "        unet_arch.to(device).eval()\n",
    "        vit_arch.to(device).eval()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model weights not found: {e}. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    indices = random.sample(range(len(test_dataset)), num_images)\n",
    "    \n",
    "    for i in indices:\n",
    "        blurry_img, sharp_img = test_dataset[i]\n",
    "        blurry_img_batch = blurry_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unet_out = unet_arch(blurry_img_batch).squeeze().cpu()\n",
    "            vit_out = vit_arch(blurry_img_batch).squeeze().cpu()\n",
    "\n",
    "        sharp_np = sharp_img.permute(1, 2, 0).numpy()\n",
    "        blurry_np = blurry_img.permute(1, 2, 0).numpy()\n",
    "        unet_np = unet_out.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        vit_np = vit_out.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "        psnr_unet = psnr(sharp_np, unet_np, data_range=1.0)\n",
    "        ssim_unet = ssim(sharp_np, unet_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_vit = psnr(sharp_np, vit_np, data_range=1.0)\n",
    "        ssim_vit = ssim(sharp_np, vit_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        \n",
    "        print(f\"--- Image Index {i} ---\")\n",
    "        print(f\"U-Net --> PSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.4f}\")\n",
    "        print(f\"ViT ----> PSNR: {psnr_vit:.2f}, SSIM: {ssim_vit:.4f}\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        axes[0].imshow(blurry_np); axes[0].set_title(\"Blurry Input\")\n",
    "        axes[1].imshow(unet_np); axes[1].set_title(f\"U-Net\\nPSNR: {psnr_unet:.2f}\")\n",
    "        axes[2].imshow(vit_np); axes[2].set_title(f\"ViT\\nPSNR: {psnr_vit:.2f}\")\n",
    "        axes[3].imshow(sharp_np); axes[3].set_title(\"Ground Truth\")\n",
    "        for ax in axes: ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# --- Run Cropped Image Evaluation ---\n",
    "unet_for_eval = smp.Unet(\"resnet50\", in_channels=3, classes=3, decoder_use_batchnorm=True, decoder_attention_type='scse')\n",
    "vit_for_eval = ViTForDeblurring(freeze_encoder=False)\n",
    "evaluate_on_cropped(\n",
    "    unet_for_eval, vit_for_eval, \n",
    "    f\"{UNET_MODEL_NAME}_deblur.pth\", f\"{VIT_MODEL_NAME}_deblur.pth\", \n",
    "    test_dataset, DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions for patch-based prediction ---\n",
    "def get_pad(h, w, patch_size, stride):\n",
    "    pad_h = (stride - (h - patch_size) % stride) % stride\n",
    "    pad_w = (stride - (w - patch_size) % stride) % stride\n",
    "    return pad_h, pad_w\n",
    "\n",
    "def predict_on_full_image(model, full_blurry_tensor, patch_size=224, overlap=32, device='cuda'):\n",
    "    model.eval()\n",
    "    full_blurry_tensor = full_blurry_tensor.unsqueeze(0).to(device)\n",
    "    _, _, h, w = full_blurry_tensor.shape\n",
    "    stride = patch_size - overlap\n",
    "    pad_h, pad_w = get_pad(h, w, patch_size, stride)\n",
    "    padded_blurry = F.pad(full_blurry_tensor, (0, pad_w, 0, pad_h), 'reflect')\n",
    "    _, _, padded_h, padded_w = padded_blurry.shape\n",
    "    full_output = torch.zeros_like(padded_blurry)\n",
    "    count_map = torch.zeros_like(padded_blurry)\n",
    "    for i in range(0, padded_h - patch_size + 1, stride):\n",
    "        for j in range(0, padded_w - patch_size + 1, stride):\n",
    "            patch = padded_blurry[:, :, i:i+patch_size, j:j+patch_size]\n",
    "            with torch.no_grad():\n",
    "                deblurred_patch = model(patch)\n",
    "            full_output[:, :, i:i+patch_size, j:j+patch_size] += deblurred_patch\n",
    "            count_map[:, :, i:i+patch_size, j:j+patch_size] += 1\n",
    "    final_output = (full_output / count_map)[:, :, :h, :w].squeeze(0).cpu()\n",
    "    return final_output\n",
    "\n",
    "def evaluate_on_full_images(unet_arch, vit_arch, unet_path, vit_path, original_test_dir, device, num_images=10):\n",
    "    \"\"\"Loads both models and evaluates on random full-resolution test images.\"\"\"\n",
    "    try:\n",
    "        unet_arch.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "        vit_arch.load_state_dict(torch.load(vit_path, weights_only=True))\n",
    "        unet_arch.to(device).eval()\n",
    "        vit_arch.to(device).eval()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model weights not found: {e}. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    sharp_dir = os.path.join(original_test_dir, 'test', 'sharp')\n",
    "    blur_dir = os.path.join(original_test_dir, 'test', 'blur')\n",
    "    \n",
    "    try:\n",
    "        image_names = os.listdir(sharp_dir)\n",
    "        selected_images = random.sample(image_names, min(num_images, len(image_names)))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Test images not found in '{sharp_dir}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    for image_name in selected_images:\n",
    "        print(f\"--- Processing Full Image: {image_name} ---\")\n",
    "        blurry_tensor = TF.to_tensor(Image.open(os.path.join(blur_dir, image_name)).convert(\"RGB\"))\n",
    "        sharp_tensor = TF.to_tensor(Image.open(os.path.join(sharp_dir, image_name)).convert(\"RGB\"))\n",
    "        \n",
    "        unet_output = predict_on_full_image(unet_arch, blurry_tensor, device=device)\n",
    "        vit_output = predict_on_full_image(vit_arch, blurry_tensor, device=device)\n",
    "\n",
    "        sharp_np = sharp_tensor.permute(1, 2, 0).numpy()\n",
    "        blurry_np = blurry_tensor.permute(1, 2, 0).numpy()\n",
    "        unet_np = unet_output.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        vit_np = vit_output.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "        psnr_unet = psnr(sharp_np, unet_np, data_range=1.0)\n",
    "        ssim_unet = ssim(sharp_np, unet_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_vit = psnr(sharp_np, vit_np, data_range=1.0)\n",
    "        ssim_vit = ssim(sharp_np, vit_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        \n",
    "        print(f\"U-Net --> PSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.4f}\")\n",
    "        print(f\"ViT ----> PSNR: {psnr_vit:.2f}, SSIM: {ssim_vit:.4f}\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        axes[0].imshow(blurry_np); axes[0].set_title(\"Blurry Input\")\n",
    "        axes[1].imshow(unet_np); axes[1].set_title(f\"U-Net\\nPSNR: {psnr_unet:.2f}\")\n",
    "        axes[2].imshow(vit_np); axes[2].set_title(f\"ViT\\nPSNR: {psnr_vit:.2f}\")\n",
    "        axes[3].imshow(sharp_np); axes[3].set_title(\"Ground Truth\")\n",
    "        for ax in axes: ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# --- Run Full Image Evaluation ---\n",
    "unet_for_eval_full = smp.Unet(\"resnet50\", in_channels=3, classes=3, decoder_use_batchnorm=True, decoder_attention_type='scse')\n",
    "vit_for_eval_full = ViTForDeblurring(freeze_encoder=False)\n",
    "evaluate_on_full_images(\n",
    "    unet_for_eval_full, vit_for_eval_full,\n",
    "    f\"{UNET_MODEL_NAME}_deblur.pth\", f\"{VIT_MODEL_NAME}_deblur.pth\",\n",
    "    ORIGINAL_DATA_DIR, DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7180_project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
