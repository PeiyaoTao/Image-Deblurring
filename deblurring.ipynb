{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6d7ce6",
   "metadata": {},
   "source": [
    "# Image Deblurring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42643d32",
   "metadata": {},
   "source": [
    "#### Authors: Peiyao Tao, Carolina Li\n",
    "09/22/2025\n",
    "\n",
    "CS 7180 Advanced Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4d336",
   "metadata": {},
   "source": [
    "The Purpose of this notebook is to compare the performance of three different image-deblurring models: U-Net, ViT, and Swin Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4a2bc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel\n",
    "from transformers import SwinModel, SwinConfig\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import segmentation_models_pytorch as smp\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import Normalize\n",
    "import requests\n",
    "import zipfile\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1872a",
   "metadata": {},
   "source": [
    "# Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eba6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = \"./DIV2K_train_HR\"\n",
    "CROPPED_DATA_DIR = \"./cropped_dataset\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded13515",
   "metadata": {},
   "source": [
    "# Data Pre-porcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045a502",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip_div2k():\n",
    "    \"\"\" Download and unzip the DIV2K dataset. \"\"\"\n",
    "    \n",
    "    # Using the DIV2K dataset\n",
    "    dataset_url = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
    "    download_dir = \"./\"\n",
    "    dataset_name = \"DIV2K_train_HR\"\n",
    "    zip_path = os.path.join(download_dir, f\"{dataset_name}.zip\")\n",
    "    \n",
    "    # Skip download if dataset already exists\n",
    "    if os.path.exists(os.path.join(download_dir, dataset_name)):\n",
    "        print(f\"Dataset '{dataset_name}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Add a progress bar to the download\n",
    "    print(f\"Downloading {dataset_name}.zip...\")\n",
    "    try:\n",
    "        response = requests.get(dataset_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        \n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "\n",
    "        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "            print(\"ERROR, something went wrong during download.\")\n",
    "            return\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Unzip the dataset\n",
    "    print(f\"\\nUnzipping {zip_path}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)\n",
    "        print(\"Unzipping complete.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The downloaded file is not a valid zip file or is corrupted.\")\n",
    "        return\n",
    "\n",
    "    # Remove the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Cleaned up {zip_path}.\")\n",
    "    print(\"--- Dataset setup is complete! ---\")\n",
    "\n",
    "download_and_unzip_div2k()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73302d",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test\n",
    "TRAIN_SHARP_DIR = os.path.join(ORIGINAL_DATA_DIR, \"train\", \"sharp\")\n",
    "TEST_SHARP_DIR = os.path.join(ORIGINAL_DATA_DIR, \"test\", \"sharp\")\n",
    "\n",
    "def organize_images():\n",
    "    \"\"\" Organize images into train and test directories. \"\"\"\n",
    "\n",
    "    # Check for existance of train and test directories\n",
    "    if os.path.exists(TRAIN_SHARP_DIR) or os.path.exists(TEST_SHARP_DIR):\n",
    "        print(\"Train/Test directories already exist. Skipping organization.\")\n",
    "        return\n",
    "\n",
    "    print(\"Organizing sharp images into train/test splits...\")\n",
    "    \n",
    "    os.makedirs(TRAIN_SHARP_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_SHARP_DIR, exist_ok=True)\n",
    "    \n",
    "    image_files = sorted([f for f in os.listdir(ORIGINAL_DATA_DIR) if f.endswith('.png')])\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"Error: No .png images found in {ORIGINAL_DATA_DIR}. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # last 100 images for testing\n",
    "    train_files = image_files[:700]\n",
    "    test_files = image_files[700:]\n",
    "\n",
    "    # Move the files\n",
    "    print(f\"Moving {len(train_files)} images to {TRAIN_SHARP_DIR}...\")\n",
    "    for filename in tqdm(train_files):\n",
    "        shutil.move(os.path.join(ORIGINAL_DATA_DIR, filename), os.path.join(TRAIN_SHARP_DIR, filename))\n",
    "        \n",
    "    print(f\"Moving {len(test_files)} images to {TEST_SHARP_DIR}...\")\n",
    "    for filename in tqdm(test_files):\n",
    "        shutil.move(os.path.join(ORIGINAL_DATA_DIR, filename), os.path.join(TEST_SHARP_DIR, filename))\n",
    "\n",
    "    print(\"Image organization complete!\")\n",
    "\n",
    "organize_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062ad31",
   "metadata": {},
   "source": [
    "## Apply Blurry Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear motion blur\n",
    "def generate_motion_blur_kernel(size, angle):\n",
    "    \"\"\" Generate a motion blur kernel. \"\"\"\n",
    "\n",
    "    kernel = np.zeros((size, size))\n",
    "    center = (size - 1) / 2\n",
    "    radian_angle = np.deg2rad(angle)\n",
    "    x_end = round(center + center * np.cos(radian_angle))\n",
    "    y_end = round(center - center * np.sin(radian_angle))\n",
    "    cv2.line(kernel, (round(center), round(center)), (int(x_end), int(y_end)), 1, thickness=1)\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "# Defocus blur\n",
    "def generate_defocus_kernel(size, radius):\n",
    "    \"\"\" Generate a defocus blur kernel. \"\"\"\n",
    "\n",
    "    kernel = np.zeros((size, size))\n",
    "    center = size // 2\n",
    "    cv2.circle(kernel, (center, center), radius, 1, thickness=-1)\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "def apply_blur_to_directory(source_dir, target_dir):\n",
    "    \"\"\" Apply random blur to all images in a directory and save them to target directory. \"\"\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(source_dir) if f.endswith(('.png'))])\n",
    "    \n",
    "    print(f\"Generating blurry images for {len(image_files)} sharp images...\")\n",
    "    \n",
    "    # Apply random blur to each image\n",
    "    for filename in tqdm(image_files):\n",
    "        sharp_path = os.path.join(source_dir, filename)\n",
    "        image = cv2.imread(sharp_path)\n",
    "        \n",
    "        blur_type = np.random.choice(['motion', 'gaussian', 'defocus'])\n",
    "        \n",
    "        if blur_type == 'motion':\n",
    "            kernel_size = np.random.randint(15, 35)\n",
    "            kernel_angle = np.random.randint(0, 180)\n",
    "            kernel = generate_motion_blur_kernel(kernel_size, kernel_angle)\n",
    "        elif blur_type == 'defocus':\n",
    "            kernel_size = np.random.randint(15, 35)\n",
    "            radius = np.random.randint(3, kernel_size // 2)\n",
    "            kernel = generate_defocus_kernel(kernel_size, radius)\n",
    "        else: # gaussian\n",
    "            kernel_size = np.random.choice([15, 19, 21, 25])\n",
    "            sigma = np.random.uniform(3, 8)\n",
    "            kernel = cv2.getGaussianKernel(kernel_size, sigma)\n",
    "            kernel = np.dot(kernel, kernel.T)\n",
    "        \n",
    "        blurry_image = cv2.filter2D(image, -1, kernel)\n",
    "        blur_path = os.path.join(target_dir, filename)\n",
    "        cv2.imwrite(blur_path, blurry_image)\n",
    "\n",
    "    print(\"Blurry images generated successfully.\")\n",
    "\n",
    "# Generate blurry images for both train and test sets if they don't already exist\n",
    "for split in ['train', 'test']:\n",
    "    sharp_folder = os.path.join(ORIGINAL_DATA_DIR, split, 'sharp')\n",
    "    blur_folder = os.path.join(ORIGINAL_DATA_DIR, split, 'blur')\n",
    "    if not os.path.exists(blur_folder) or len(os.listdir(blur_folder)) == 0:\n",
    "        print(f\"Generating blurry images for '{split}' set...\")\n",
    "        apply_blur_to_directory(sharp_folder, blur_folder)\n",
    "    else:\n",
    "        print(f\"Blurry images for '{split}' set already exist. Skipping generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c4850",
   "metadata": {},
   "source": [
    "## Random Image Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops(source_base, target_base, crop_size=(224, 224), crops_per_image=10):\n",
    "    \"\"\" Create random crops from images to form our training dataset. \"\"\"\n",
    "\n",
    "    # Skips cropping if target directory already exists and is not empty\n",
    "    if os.path.exists(target_base) and len(os.listdir(target_base)) > 0:\n",
    "        print(\"Cropped dataset directory already exists and is not empty. Skipping cropping.\")\n",
    "        return\n",
    "        \n",
    "    print(\"Starting dataset pre-processing with random crops...\")\n",
    "\n",
    "    # Crop 10 images from each original image\n",
    "    for split in ['train', 'test']:\n",
    "        sharp_source_dir = os.path.join(source_base, split, 'sharp')\n",
    "        blur_source_dir = os.path.join(source_base, split, 'blur')\n",
    "\n",
    "        if not os.path.exists(sharp_source_dir):\n",
    "            continue\n",
    "\n",
    "        sharp_target_dir = os.path.join(target_base, split, 'sharp')\n",
    "        blur_target_dir = os.path.join(target_base, split, 'blur')\n",
    "        os.makedirs(sharp_target_dir, exist_ok=True)\n",
    "        os.makedirs(blur_target_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing images in: {sharp_source_dir}\")\n",
    "        for filename in tqdm(os.listdir(sharp_source_dir)):\n",
    "            sharp_img = Image.open(os.path.join(sharp_source_dir, filename)).convert(\"RGB\")\n",
    "            blur_img = Image.open(os.path.join(blur_source_dir, filename)).convert(\"RGB\")\n",
    "\n",
    "            img_w, img_h = sharp_img.size\n",
    "            crop_h, crop_w = crop_size\n",
    "            if img_w < crop_w or img_h < crop_h:\n",
    "                continue\n",
    "\n",
    "            for i in range(crops_per_image):\n",
    "                top, left = random.randint(0, img_h - crop_h), random.randint(0, img_w - crop_w)\n",
    "                sharp_cropped = sharp_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "                blur_cropped = blur_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "                \n",
    "                base_name, ext = os.path.splitext(filename)\n",
    "                new_filename = f\"{base_name}_crop_{i}{ext}\"\n",
    "                sharp_cropped.save(os.path.join(sharp_target_dir, new_filename))\n",
    "                blur_cropped.save(os.path.join(blur_target_dir, new_filename))\n",
    "    print(f\"Cropped images saved to: {target_base}\")\n",
    "\n",
    "create_crops(ORIGINAL_DATA_DIR, CROPPED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e104fed",
   "metadata": {},
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed50532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split 10% of training datat into validation set\n",
    "def create_validation_split(base_dir, ratio=0.1):\n",
    "    \"\"\" Create a validation split from the training data. \"\"\"\n",
    "\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    val_dir = os.path.join(base_dir, \"validation\")\n",
    "    \n",
    "    if os.path.exists(val_dir):\n",
    "        print(\"Validation directory already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating validation split...\")\n",
    "    os.makedirs(os.path.join(val_dir, \"sharp\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, \"blur\"), exist_ok=True)\n",
    "\n",
    "    sharp_train_dir = os.path.join(train_dir, \"sharp\")\n",
    "    image_files = sorted(os.listdir(sharp_train_dir))\n",
    "    random.shuffle(image_files)\n",
    "    num_val_images = int(len(image_files) * ratio)\n",
    "    val_images = image_files[:num_val_images]\n",
    "\n",
    "    print(f\"Moving {num_val_images} images from train to validation set...\")\n",
    "    for filename in tqdm(val_images):\n",
    "        shutil.move(os.path.join(train_dir, \"sharp\", filename), os.path.join(val_dir, \"sharp\", filename))\n",
    "        shutil.move(os.path.join(train_dir, \"blur\", filename), os.path.join(val_dir, \"blur\", filename))\n",
    "    print(\"Validation set created successfully.\")\n",
    "\n",
    "create_validation_split(CROPPED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb01f18",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class DeblurDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        self.split_dir = os.path.join(root_dir, split)\n",
    "        self.sharp_dir = os.path.join(self.split_dir, 'sharp')\n",
    "        self.blur_dir = os.path.join(self.split_dir, 'blur')\n",
    "        self.image_files = sorted(os.listdir(self.sharp_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        sharp_path = os.path.join(self.sharp_dir, img_name)\n",
    "        blur_path = os.path.join(self.blur_dir, img_name)\n",
    "        \n",
    "        sharp_image = Image.open(sharp_path).convert(\"RGB\")\n",
    "        blur_image = Image.open(blur_path).convert(\"RGB\")\n",
    "\n",
    "        # Data augmentation by applying random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            sharp_image = TF.hflip(sharp_image)\n",
    "            blur_image = TF.hflip(blur_image)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sharp_image = TF.to_tensor(sharp_image)\n",
    "        blur_image = TF.to_tensor(blur_image)\n",
    "        \n",
    "        return blur_image, sharp_image\n",
    "    \n",
    "train_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='train')\n",
    "val_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='validation')\n",
    "test_dataset = DeblurDataset(root_dir=CROPPED_DATA_DIR, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(f\"Training dataset loaded with {len(train_dataset)} images.\")\n",
    "print(f\"Validation dataset loaded with {len(val_dataset)} images.\")\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867880db",
   "metadata": {},
   "source": [
    "# Model Design & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79049972",
   "metadata": {},
   "source": [
    "## Custom Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ECA module and ConvBlock with ECA for ViT and Swin Transformer\n",
    "class ECA(nn.Module):\n",
    "    # For simplicity, we use a fixed kernel size of 3\n",
    "    def __init__(self, channels, k_size=3):\n",
    "        super().__init__()\n",
    "        padding = (k_size - 1) // 2\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        y = self.avg_pool(x) \n",
    "        y = y.squeeze(-1).squeeze(-1) \n",
    "        y = y.unsqueeze(1)\n",
    "        y = self.conv(y) \n",
    "        y = y.squeeze(1)\n",
    "        y = self.sigmoid(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.eca = ECA(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.eca(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7059704",
   "metadata": {},
   "source": [
    "## ViT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Decoder with skip connections\n",
    "class ViTDecoderWithSkips(nn.Module):\n",
    "    def __init__(self, in_features=768, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Upsampling\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_features, 256, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv1 = ConvBlock(256 + 768, 256)\n",
    "        \n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv2 = ConvBlock(128 + 768, 128)\n",
    "\n",
    "        self.upconv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv3 = ConvBlock(64 + 768, 64)\n",
    "\n",
    "        self.upconv4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv4 = ConvBlock(32, 32)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    # Forward method with skip connections\n",
    "    def forward(self, x, skips):\n",
    "        patches = x[:, 1:, :]\n",
    "        \n",
    "        h = w = int(patches.shape[1]**0.5)\n",
    "        x = patches.permute(0, 2, 1).contiguous().view(-1, 768, h, w)\n",
    "\n",
    "        skip1 = skips[0][:, 1:, :].permute(0, 2, 1).contiguous().view(-1, 768, h, w)\n",
    "        skip2 = skips[1][:, 1:, :].permute(0, 2, 1).contiguous().view(-1, 768, h, w)\n",
    "        skip3 = skips[2][:, 1:, :].permute(0, 2, 1).contiguous().view(-1, 768, h, w)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        skip1_up = F.interpolate(skip1, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, skip1_up], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        skip2_up = F.interpolate(skip2, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, skip2_up], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        skip3_up = F.interpolate(skip3, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x, skip3_up], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Full ViT Model\n",
    "class ViTForDeblurring(nn.Module):\n",
    "    def __init__(self, freeze_encoder=True, num_unfrozen_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k', output_hidden_states=True)\n",
    "        self.decoder = ViTDecoderWithSkips()\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if num_unfrozen_layers > 0:\n",
    "            print(f\"Fine-tuning ViT: Unfreezing the last {num_unfrozen_layers} transformer layers.\")\n",
    "            for layer in self.encoder.encoder.layer[-num_unfrozen_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            for param in self.encoder.layernorm.parameters():\n",
    "                 param.requires_grad = True\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, x):\n",
    "        encoder_output = self.encoder(x)\n",
    "        all_hidden_states = encoder_output.hidden_states\n",
    "        skips = [\n",
    "            all_hidden_states[3],\n",
    "            all_hidden_states[6],\n",
    "            all_hidden_states[9]\n",
    "        ]\n",
    "        final_state = all_hidden_states[-1]\n",
    "        decoded_output = self.decoder(final_state, skips)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d49a5f",
   "metadata": {},
   "source": [
    "## Swin Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481784df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swin Transformer Model with a U-Net style decoder\n",
    "class SwinUnetDecoder(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(768, 384, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(384, 192, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.upconv3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(192, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.upconv4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv1 = ConvBlock(in_channels=384 * 2, out_channels=384)\n",
    "        self.conv2 = ConvBlock(in_channels=192 * 2, out_channels=192)\n",
    "        self.conv3 = ConvBlock(in_channels=96 * 2, out_channels=96)\n",
    "        self.conv4 = ConvBlock(in_channels=64, out_channels=32)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, encoder_features):\n",
    "        e1, e2, e3, e4 = encoder_features\n",
    "\n",
    "        x = self.upconv1(e4)\n",
    "        x = torch.cat([x, e3], dim=1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, e2], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, e1], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "# Full Swin Model\n",
    "class SwinForDeblurring(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = SwinConfig.from_pretrained('microsoft/swin-tiny-patch4-window7-224', output_hidden_states=True)\n",
    "        self.encoder = SwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224', config=config)\n",
    "        self.decoder = SwinUnetDecoder()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, x):\n",
    "        hidden_states = self.encoder(x).hidden_states\n",
    "\n",
    "        encoder_features = []\n",
    "        for hs in hidden_states[:4]:\n",
    "            b, n, c = hs.shape \n",
    "            h = int(n ** 0.5)  \n",
    "            w = n // h\n",
    "\n",
    "            hs_reshaped = hs.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
    "            encoder_features.append(hs_reshaped)\n",
    "\n",
    "        # Use residual learning\n",
    "        residual = self.decoder(encoder_features)\n",
    "        return torch.clamp(x + residual, 0, 1)\n",
    "\n",
    "print(\"\\n Swin-Unet model with improved upsampling defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb65212",
   "metadata": {},
   "source": [
    "## VGG Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual Loss using VGG19\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:18].eval()\n",
    "        self.features = nn.Sequential(*vgg).to(DEVICE)\n",
    "\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, input_img, target_img):\n",
    "        input_norm = self.normalize(input_img)\n",
    "        target_norm = self.normalize(target_img)\n",
    "\n",
    "        input_features = self.features(input_norm)\n",
    "        target_features = self.features(target_norm)\n",
    "\n",
    "        return self.l1(input_features, target_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f9b0e",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class with model checkpointing\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, best_model_path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_model_path = best_model_path\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.best_model_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping!\")\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main training function\n",
    "def train_model(model, model_name, train_loader, val_loader, optimizer, epochs, device, lambda_vgg=0.05, lambda_ssim=2.0):\n",
    "    # Loss functions and metrics\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_vgg = VGGPerceptualLoss(device).to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    model.to(device)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    best_model_path = f'{model_name}_best.pth'\n",
    "    early_stopper = EarlyStopping(patience=10, best_model_path=best_model_path)\n",
    "\n",
    "    scaler = GradScaler(device=\"cuda\")\n",
    "    \n",
    "    print(f\"--- Starting Training for {model_name} (Best model will be saved to {best_model_path}) ---\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_l1_loss, running_vgg_loss, running_ssim_loss = 0.0, 0.0, 0.0\n",
    "        for blurry_imgs, sharp_imgs in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            blurry_imgs, sharp_imgs = blurry_imgs.to(device), sharp_imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = model(blurry_imgs)\n",
    "                loss_l1 = criterion_l1(outputs, sharp_imgs)\n",
    "                loss_vgg = criterion_vgg(outputs, sharp_imgs)\n",
    "                loss_ssim = 1 - ssim(outputs, sharp_imgs)\n",
    "                total_loss = loss_l1 + (lambda_vgg * loss_vgg) + (lambda_ssim * loss_ssim)\n",
    "\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_l1_loss += loss_l1.item()\n",
    "            running_vgg_loss += loss_vgg.item()\n",
    "            running_ssim_loss += loss_ssim.item()\n",
    "        \n",
    "        avg_train_l1 = running_l1_loss / len(train_loader)\n",
    "        avg_train_vgg = running_vgg_loss / len(train_loader)\n",
    "        avg_train_ssim = running_ssim_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss_l1 = 0.0\n",
    "        val_loss_vgg = 0.0\n",
    "        val_loss_ssim = 0.0\n",
    "        with torch.no_grad():\n",
    "            for blurry_imgs, sharp_imgs in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                blurry_imgs, sharp_imgs = blurry_imgs.to(device), sharp_imgs.to(device)\n",
    "                outputs = model(blurry_imgs)\n",
    "                loss_l1 = criterion_l1(outputs, sharp_imgs)\n",
    "                loss_vgg = criterion_vgg(outputs, sharp_imgs)\n",
    "                loss_ssim = 1 - ssim(outputs, sharp_imgs)\n",
    "                val_loss_l1 += loss_l1.item()\n",
    "                val_loss_vgg += loss_vgg.item()\n",
    "                val_loss_ssim += loss_ssim.item()\n",
    "\n",
    "        avg_val_loss_l1 = val_loss_l1 / len(val_loader)\n",
    "        avg_val_loss_vgg = val_loss_vgg / len(val_loader)\n",
    "        avg_val_loss_ssim = val_loss_ssim / len(val_loader)\n",
    "        avg_val_ssim_score = 1 - avg_val_loss_ssim\n",
    "        \n",
    "        current_learn_rate = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] complete. Train L1: {avg_train_l1:.4f}, Train VGG: {avg_train_vgg:.4f}, \"\n",
    "            f\"Train SSIM: {avg_train_ssim:.4f}, Val L1: {avg_val_loss_l1:.4f}, Val VGG: {avg_val_loss_vgg:.4f}, \"\n",
    "            f\"Val SSIM: {avg_val_ssim_score:.4f}, LR: {current_learn_rate:.6f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss_l1)\n",
    "        early_stopper(avg_val_loss_l1, avg_val_loss_vgg, avg_val_loss_ssim, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered. Exiting training loop.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"--- Finished Training for {model_name} ---\")\n",
    "    print(f\"Loading best model weights from {best_model_path} (Val Loss: {early_stopper.best_loss:.6f})\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38169583",
   "metadata": {},
   "source": [
    "## Model Setup & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_attention_type='scse'\n",
    ")\n",
    "\n",
    "optimizer_unet = optim.AdamW(unet_model.parameters(), lr=LEARNING_RATE)\n",
    "UNET_MODEL_NAME = \"unet_best\"\n",
    "print(\"U-Net with ResNet50 backbone created successfully.\")\n",
    "\n",
    "# ViT\n",
    "vit_model = ViTForDeblurring(freeze_encoder=False)\n",
    "optimizer_vit = optim.Adam([\n",
    "    {'params': vit_model.encoder.parameters(), 'lr': 1e-6},  # Lower LR for encoder\n",
    "    {'params': vit_model.decoder.parameters(), 'lr': 1e-4}   # Higher LR for decoder\n",
    "])\n",
    "VIT_MODEL_NAME = \"vit_final_best\"\n",
    "print(\"ViT-based model created successfully.\")\n",
    "\n",
    "# Swin Transformer\n",
    "swin_model = SwinForDeblurring()\n",
    "optimizer_swin = optim.AdamW(swin_model.parameters(), lr=LEARNING_RATE)\n",
    "SWIN_MODEL_NAME = \"swin_final_best\"\n",
    "print(\"Swin Transformer model created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5932422",
   "metadata": {},
   "source": [
    "### UNet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_unet = train_model(unet_model, UNET_MODEL_NAME, train_loader, val_loader, optimizer_unet, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c010d1c",
   "metadata": {},
   "source": [
    "### ViT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vit = train_model(vit_model, VIT_MODEL_NAME, train_loader, val_loader, optimizer_vit, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc37045",
   "metadata": {},
   "source": [
    "### Swin Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b34874",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_swin = train_model(swin_model, SWIN_MODEL_NAME, train_loader, val_loader, optimizer_swin, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951992f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa029249",
   "metadata": {},
   "source": [
    "## Evaluation On Cropped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78493841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_cropped(unet_arch, vit_arch, swin_arch, unet_path, vit_path, swin_path, test_dataset, device, num_images=10):\n",
    "    \"\"\" Evaluate models on cropped test dataset and display results with PSNR and SSIM metrics. \"\"\"\n",
    "    try:\n",
    "        unet_arch.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "        vit_arch.load_state_dict(torch.load(vit_path, weights_only=True))\n",
    "        swin_arch.load_state_dict(torch.load(swin_path, weights_only=True))\n",
    "        unet_arch.to(device).eval()\n",
    "        vit_arch.to(device).eval()\n",
    "        swin_arch.to(device).eval()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model weights not found: {e}. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    indices = random.sample(range(len(test_dataset)), num_images)\n",
    "    for i in indices:\n",
    "        blurry_img, sharp_img = test_dataset[i]\n",
    "        blurry_img_batch = blurry_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unet_out = unet_arch(blurry_img_batch).squeeze().cpu()\n",
    "            vit_out = vit_arch(blurry_img_batch).squeeze().cpu()\n",
    "            swin_out = swin_arch(blurry_img_batch).squeeze().cpu()\n",
    "\n",
    "        sharp_np = sharp_img.permute(1, 2, 0).numpy()\n",
    "        blurry_np = blurry_img.permute(1, 2, 0).numpy()\n",
    "        unet_np = unet_out.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        vit_np = vit_out.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        swin_np = swin_out.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "        psnr_unet, ssim_unet = psnr(sharp_np, unet_np, data_range=1.0), ssim(sharp_np, unet_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_vit, ssim_vit = psnr(sharp_np, vit_np, data_range=1.0), ssim(sharp_np, vit_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_swin, ssim_swin = psnr(sharp_np, swin_np, data_range=1.0), ssim(sharp_np, swin_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        \n",
    "        print(f\"--- Image Index {i} ---\")\n",
    "        print(f\"U-Net --> PSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.4f}\")\n",
    "        print(f\"ViT ----> PSNR: {psnr_vit:.2f}, SSIM: {ssim_vit:.4f}\")\n",
    "        print(f\"Swin ---> PSNR: {psnr_swin:.2f}, SSIM: {ssim_swin:.4f}\")\n",
    "\n",
    "        # Plotting the results\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "        axes[0].imshow(blurry_np)\n",
    "        axes[0].set_title(\"Blurry Input\")\n",
    "\n",
    "        axes[1].imshow(unet_np)\n",
    "        axes[1].set_title(f\"U-Net\\nPSNR: {psnr_unet:.2f} | SSIM: {ssim_unet:.4f}\")\n",
    "\n",
    "        axes[2].imshow(vit_np)\n",
    "        axes[2].set_title(f\"ViT\\nPSNR: {psnr_vit:.2f} | SSIM: {ssim_vit:.4f}\")\n",
    "        \n",
    "        axes[3].imshow(swin_np)\n",
    "        axes[3].set_title(f\"Swin\\nPSNR: {psnr_swin:.2f} | SSIM: {ssim_swin:.4f}\")\n",
    "\n",
    "        axes[4].imshow(sharp_np)\n",
    "        axes[4].set_title(\"Ground Truth\")\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "unet_for_eval = smp.Unet(\"resnet50\", in_channels=3, classes=3, decoder_use_batchnorm=True, decoder_attention_type='scse')\n",
    "vit_for_eval = ViTForDeblurring(num_unfrozen_layers=4)\n",
    "swin_for_eval = SwinForDeblurring()\n",
    "evaluate_on_cropped(\n",
    "    unet_for_eval, vit_for_eval, swin_for_eval,\n",
    "    f\"{UNET_MODEL_NAME}.pth\", f\"{VIT_MODEL_NAME}.pth\", f\"{SWIN_MODEL_NAME}.pth\",\n",
    "    test_dataset, DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ebfbc",
   "metadata": {},
   "source": [
    "## Evaluation On Full Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad(h, w, patch_size, stride):\n",
    "    \"\"\" Helper function to calculate padding needed for height and width. \"\"\"\n",
    "    pad_h = (stride - (h - patch_size) % stride) % stride\n",
    "    pad_w = (stride - (w - patch_size) % stride) % stride\n",
    "    return pad_h, pad_w\n",
    "\n",
    "def predict_on_full_image(model, full_blurry_tensor, patch_size=224, overlap=32, device='cuda'):\n",
    "    \"\"\" Predict on a full image by dividing it into overlapping patches. \"\"\"\n",
    "    model.eval()\n",
    "    full_blurry_tensor = full_blurry_tensor.unsqueeze(0).to(device)\n",
    "    _, _, h, w = full_blurry_tensor.shape\n",
    "    stride = patch_size - overlap\n",
    "    pad_h, pad_w = get_pad(h, w, patch_size, stride)\n",
    "    padded_blurry = F.pad(full_blurry_tensor, (0, pad_w, 0, pad_h), 'reflect')\n",
    "    _, _, padded_h, padded_w = padded_blurry.shape\n",
    "    full_output = torch.zeros_like(padded_blurry)\n",
    "    count_map = torch.zeros_like(padded_blurry)\n",
    "    for i in range(0, padded_h - patch_size + 1, stride):\n",
    "        for j in range(0, padded_w - patch_size + 1, stride):\n",
    "            patch = padded_blurry[:, :, i:i+patch_size, j:j+patch_size]\n",
    "            with torch.no_grad():\n",
    "                deblurred_patch = model(patch)\n",
    "            full_output[:, :, i:i+patch_size, j:j+patch_size] += deblurred_patch\n",
    "            count_map[:, :, i:i+patch_size, j:j+patch_size] += 1\n",
    "    final_output = (full_output / count_map)[:, :, :h, :w].squeeze(0).cpu()\n",
    "    return final_output\n",
    "\n",
    "def evaluate_on_full_images(unet_arch, vit_arch, swin_arch, unet_path, vit_path, swin_path, original_test_dir, device, num_images=100):\n",
    "    \"\"\" Evaluate models on full-size test images and display results with PSNR and SSIM metrics. \"\"\"\n",
    "    try:\n",
    "        unet_arch.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "        vit_arch.load_state_dict(torch.load(vit_path, weights_only=True))\n",
    "        swin_arch.load_state_dict(torch.load(swin_path, weights_only=True))\n",
    "        unet_arch.to(device).eval()\n",
    "        vit_arch.to(device).eval()\n",
    "        swin_arch.to(device).eval()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model weights not found: {e}. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    sharp_dir = os.path.join(original_test_dir, 'test', 'sharp')\n",
    "    blur_dir = os.path.join(original_test_dir, 'test', 'blur')\n",
    "    \n",
    "    try:\n",
    "        image_names = os.listdir(sharp_dir)\n",
    "        selected_images = random.sample(image_names, min(num_images, len(image_names)))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Test images not found in '{sharp_dir}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    for image_name in selected_images:\n",
    "        print(f\"--- Processing Full Image: {image_name} ---\")\n",
    "        blurry_tensor = TF.to_tensor(Image.open(os.path.join(blur_dir, image_name)).convert(\"RGB\"))\n",
    "        sharp_tensor = TF.to_tensor(Image.open(os.path.join(sharp_dir, image_name)).convert(\"RGB\"))\n",
    "        \n",
    "        unet_output = predict_on_full_image(unet_arch, blurry_tensor, device=device)\n",
    "        vit_output = predict_on_full_image(vit_arch, blurry_tensor, device=device)\n",
    "        swin_output = predict_on_full_image(swin_arch, blurry_tensor, device=device)\n",
    "\n",
    "        sharp_np = sharp_tensor.permute(1, 2, 0).numpy()\n",
    "        blurry_np = blurry_tensor.permute(1, 2, 0).numpy()\n",
    "        unet_np = unet_output.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        vit_np = vit_output.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        swin_np = swin_output.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "        psnr_unet, ssim_unet = psnr(sharp_np, unet_np, data_range=1.0), ssim(sharp_np, unet_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_vit, ssim_vit = psnr(sharp_np, vit_np, data_range=1.0), ssim(sharp_np, vit_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "        psnr_swin, ssim_swin = psnr(sharp_np, swin_np, data_range=1.0), ssim(sharp_np, swin_np, multichannel=True, data_range=1.0, channel_axis=-1)\n",
    "\n",
    "        print(f\"U-Net --> PSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.4f}\")\n",
    "        print(f\"ViT ----> PSNR: {psnr_vit:.2f}, SSIM: {ssim_vit:.4f}\")\n",
    "        print(f\"Swin ---> PSNR: {psnr_swin:.2f}, SSIM: {ssim_swin:.4f}\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(24, 16))\n",
    "\n",
    "        # --- Row 1: Input and Ground Truth ---\n",
    "        axes[0, 0].imshow(blurry_np)\n",
    "        axes[0, 0].set_title(\"Blurry Input\")\n",
    "\n",
    "        axes[0, 1].imshow(sharp_np)\n",
    "        axes[0, 1].set_title(\"Ground Truth\")\n",
    "        \n",
    "        # Turn off the unused subplot in the first row\n",
    "        axes[0, 2].axis('off')\n",
    "\n",
    "        # --- Row 2: Model Outputs ---\n",
    "        axes[1, 0].imshow(unet_np)\n",
    "        axes[1, 0].set_title(f\"U-Net\\nPSNR: {psnr_unet:.2f} | SSIM: {ssim_unet:.4f}\")\n",
    "\n",
    "        axes[1, 1].imshow(vit_np)\n",
    "        axes[1, 1].set_title(f\"ViT\\nPSNR: {psnr_vit:.2f} | SSIM: {ssim_vit:.4f}\")\n",
    "\n",
    "        axes[1, 2].imshow(swin_np)\n",
    "        axes[1, 2].set_title(f\"Swin\\nPSNR: {psnr_swin:.2f} | SSIM: {ssim_swin:.4f}\")\n",
    "\n",
    "        # Iterate over all subplots in the grid to turn off axis lines/ticks\n",
    "        for ax in axes.flat:\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "unet_for_eval = smp.Unet(\"resnet50\", in_channels=3, classes=3, decoder_use_batchnorm=True, decoder_attention_type='scse')\n",
    "vit_for_eval = ViTForDeblurring(num_unfrozen_layers=4)\n",
    "swin_for_eval = SwinForDeblurring()\n",
    "\n",
    "evaluate_on_full_images(\n",
    "    unet_for_eval, vit_for_eval, swin_for_eval,\n",
    "    f\"{UNET_MODEL_NAME}.pth\", f\"{VIT_MODEL_NAME}.pth\", f\"{SWIN_MODEL_NAME}.pth\",\n",
    "    ORIGINAL_DATA_DIR, DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ac2b8",
   "metadata": {},
   "source": [
    "## Compare With Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(unet_arch, vit_arch, swin_arch, unet_path, vit_path, swin_path, test_loader, device):\n",
    "    \"\"\" Calculates average metrics over the test set. \"\"\"\n",
    "\n",
    "    try:\n",
    "        unet_arch.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "        vit_arch.load_state_dict(torch.load(vit_path, weights_only=True))\n",
    "        swin_arch.load_state_dict(torch.load(swin_path, weights_only=True))\n",
    "        \n",
    "        unet_arch.to(device).eval()\n",
    "        vit_arch.to(device).eval()\n",
    "        swin_arch.to(device).eval()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model weights not found: {e}. Skipping final evaluation.\")\n",
    "        return\n",
    "\n",
    "    unet_psnrs, unet_ssims = [], []\n",
    "    vit_psnrs, vit_ssims = [], []\n",
    "    swin_psnrs, swin_ssims = [], []\n",
    "\n",
    "    print(\"Calculating average metrics over the entire test set...\")\n",
    "    with torch.no_grad():\n",
    "        for blurry_imgs, sharp_imgs in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
    "            blurry_imgs = blurry_imgs.to(device)\n",
    "            \n",
    "            # Process U-Net\n",
    "            unet_outputs = unet_arch(blurry_imgs).cpu() \n",
    "            \n",
    "            # Process ViT\n",
    "            vit_outputs = vit_arch(blurry_imgs).cpu()\n",
    "\n",
    "            # Process Swin\n",
    "            swin_outputs = swin_arch(blurry_imgs).cpu()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Iterate over the batch on the CPU\n",
    "            for i in range(sharp_imgs.shape[0]):\n",
    "                sharp_np = sharp_imgs[i].permute(1, 2, 0).numpy()\n",
    "                \n",
    "                unet_np = unet_outputs[i].permute(1, 2, 0).numpy().clip(0, 1)\n",
    "                unet_psnrs.append(psnr(sharp_np, unet_np, data_range=1.0))\n",
    "                unet_ssims.append(ssim(sharp_np, unet_np, multichannel=True, data_range=1.0, channel_axis=-1))\n",
    "\n",
    "                vit_np = vit_outputs[i].permute(1, 2, 0).numpy().clip(0, 1)\n",
    "                vit_psnrs.append(psnr(sharp_np, vit_np, data_range=1.0))\n",
    "                vit_ssims.append(ssim(sharp_np, vit_np, multichannel=True, data_range=1.0, channel_axis=-1))\n",
    "\n",
    "                swin_np = swin_outputs[i].permute(1, 2, 0).numpy().clip(0, 1)\n",
    "                swin_psnrs.append(psnr(sharp_np, swin_np, data_range=1.0))\n",
    "                swin_ssims.append(ssim(sharp_np, swin_np, multichannel=True, data_range=1.0, channel_axis=-1))\n",
    "\n",
    "    print(\"\\n--- Final Average Metrics ---\")\n",
    "    print(f\"U-Net --> Average PSNR: {np.mean(unet_psnrs):.2f}, Average SSIM: {np.mean(unet_ssims):.4f}\")\n",
    "    print(f\"ViT ----> Average PSNR: {np.mean(vit_psnrs):.2f}, Average SSIM: {np.mean(vit_ssims):.4f}\")\n",
    "    print(f\"Swin ---> Average PSNR: {np.mean(swin_psnrs):.2f}, Average SSIM: {np.mean(swin_ssims):.4f}\")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "unet_for_eval = smp.Unet(\"resnet50\", in_channels=3, classes=3, decoder_use_batchnorm=True, decoder_attention_type='scse')\n",
    "vit_for_eval = ViTForDeblurring(num_unfrozen_layers=4)\n",
    "swin_for_eval = SwinForDeblurring()\n",
    "\n",
    "\n",
    "calculate_average_metrics(\n",
    "    unet_for_eval, vit_for_eval, swin_for_eval,\n",
    "    f\"{UNET_MODEL_NAME}.pth\", \n",
    "    f\"{VIT_MODEL_NAME}.pth\", \n",
    "    f\"{SWIN_MODEL_NAME}.pth\",\n",
    "    test_loader, \n",
    "    DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7180_project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
